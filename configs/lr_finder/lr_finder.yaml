# @package _global_

defaults:
  - _self_

  - m_preset: mlp_relu_raw
  - optimizer: adam


path_to_dataset: ${paths.root_dir}/div_check/data/tal_noisemaker_mn04_size=65536_seed=45858_2023-12-13_14-19-35

# number of times the lr_finder should be run. Each run will increment the seed by 1.
num_run: 10

# seed for the first run (will be incremented by 1 for each subsequent run)
start_seed: 666

loader:
  batch_size: 64
  num_workers: 8
  shuffle: True

solver:
  _target_: hpo.lit_module.PresetEmbeddingHPO
  _convert_: all # somehow resolves an omegaconf missing key error during trainer.fit setup
  loss:
    _target_: torch.nn.L1Loss

  # optimizer:

lr_finder:
  # pass `exponential` to increase the LR exponentially or 'linear' to increase it linearly.
  mode: exponential 
  
  # min LR to investigate
  min_lr: 1e-5 
  
  # max LR to investigate
  max_lr: 1e-2 

  # number of LRs to test
  num_training: 100 
  
  # Threshold for stopping the search. 
  # If the loss at any point is larger than early_stop_threshold*best_loss then the search is stopped. 
  # To disable, set to null.
  early_stop_threshold: 4 
        
############ Hydra stuff
# disable output directory
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${paths.root_dir}/logs/lr_finder/${m_preset.name}_${optimizer.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir:  ${paths.root_dir}/logs/lr_finder/${m_preset.name}_${optimizer.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: bs=${loader.batch_size}_nb=${m_preset.cfg.num_blocks}_hf=${m_preset.cfg.hidden_features}

  job: 
    chdir: True

  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    formatters:
      simple:
        format: '%(message)s'
