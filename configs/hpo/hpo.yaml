# @package _global_

defaults:
  - _self_
  - m_preset: mlp_relu_raw
  - sampler: tpe
  - pruner: nop
  - wandb: default

# TODO: export train dataset change path 
# path_to_train_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=1000000_seed=300_pkl_hpt-train_1
path_to_train_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=65536_seed=400_pkl_hpt-val_1
path_to_val_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=65536_seed=400_pkl_hpt-val_1

# seed for the first run (will be incremented by 1 for each subsequent run)
seed: 42

# number of trials to run
num_trials: 100

# in which direction the objective should be optimized (minimize or maximize)
direction: maximize

# metric to optimize (for optuna lightning callback)
metric_to_optimize: val/mrr

# name of the optuna study
study_name: ${m_preset.name}_${sampler.name}_${pruner.name}

# number of workers for the train and val dataloader
num_workers: 8
num_ranks_mrr: 128

# batch size (not optimized, allows to better analyze the training loss curve for a given study)
batch_size: ???

# # whether to tune the lr warmup or not
# tune_lr_warmup: True

# # scheduler config for lightning module
# scheduler_config: 
#   interval: step
#   frequency: 1

############ Hyperparameters search space (to be passed to optuna.trial.suggest_*)
# batch size used for training (suggest_categorical)
search_space:
  # train_batch_size: 
  #   - 32
  #   - 64
  #   - 128
  #   - 256
  #   - 512
  #   - 1024
  #   - 2048

  # min/max number of blocks for the preset encoder (suggest_int, linear scale)
  num_blocks: 
    - 1
    - 6

  # min/max number of hidden features for the preset encoder (suggest_int, log scale)
  hidden_features: 
    - 256
    - 2048

  # optimizer (suggest_categorical)
  optimizer_names:
    - Adam
    - NAdam

  # min/max learning rate (suggest_float, log scale)
  learning_rate: 
    - 0.0001 
    - 0.005

  # # LR warmup: min/max total number of warmup steps (suggest_int, linear scale)
  # lr_warmup_total_iters: 
  #   - 0
  #   - 1000

  # # LR warmup: min/max start factor (suggest_float, linear scale)
  # lr_warmup_start_factor: 
  #   - 0.05
  #   - 0.6

############ Hydra stuff
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${paths.root_dir}/logs/optuna/${m_preset.name}_${sampler.name}_${pruner.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir:  ${paths.root_dir}/logs/optuna/${m_preset.name}_${sampler.name}_${pruner.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}

  job: 
    chdir: True

  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    formatters:
      simple:
        format: '%(message)s'
