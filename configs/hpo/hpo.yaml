# @package _global_

defaults:
  - _self_
  - m_preset: mlp_relu_raw
  - sampler: tpe
  - pruner: nop
  - wandb: default

# TODO: export train dataset change path 
# path_to_train_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=1000000_seed=300_pkl_hpt-train_1
path_to_train_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=65536_seed=400_pkl_hpt-val_1
path_to_val_dataset: ${paths.root_dir}/data/datasets/tal_noisemaker_mn04_size=65536_seed=400_pkl_hpt-val_1

# seed for the first run (will be incremented by 1 for each subsequent run)
seed: 42

# number of trials to run
num_trials: 100

# in which direction the objective should be optimized (minimize or maximize)
direction: maximize

# metric to optimize (for optuna lightning callback)
metric_to_optimize: val/mrr

# number of workers for the train and val dataloader
num_workers: 8
num_ranks_mrr: 128

# kwargs for lightning trainer
trainer:
  max_epochs: 100
  log_every_n_steps: 50


############ Hyperparameters search space
# batch size used for training
search_space:
  batch_size: 
    type: categorical
    kwargs:
      name: batch_size
      choices:
        - 32
        # - 64
        # - 128
        # - 256
        # - 512
        # - 1024
        # - 2048

  # number of blocks for the preset encoder (linear scale)
  num_blocks: 
    type: int
    kwargs:
      name: num_blocks
      low: 1
      high: 6

  # number of hidden features for the preset encoder (linear scale)
  hidden_features: 
    type: int
    kwargs:
      name: hidden_features
      low: 256
      high: 4096
      step: 256

  # optimizer
  optimizer:
    type: categorical
    kwargs:
      name: optimizer
      choices:
        - Adam
        # - NAdam

  # learning rate (log scale)
  learning_rate: 
    type: float
    kwargs:
      name: learning_rate
      low: 0.0001
      high: 0.01
      log: true

############ Artifacts
# name of the optuna study
study_name: results_${now:%Y-%m-%d}_${now:%H-%M-%S}

############ Hydra stuff
paths:
  root_dir: ${oc.env:PROJECT_ROOT}
  output_dir: ${hydra:runtime.output_dir}

hydra:
  run:
    dir: ${paths.root_dir}/logs/optuna/${m_preset.name}_${sampler.name}_${pruner.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir:  ${paths.root_dir}/logs/optuna/${m_preset.name}_${sampler.name}_${pruner.name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}

  job: 
    chdir: True

  job_logging:
    handlers:
      file:
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    formatters:
      simple:
        format: '%(message)s'
