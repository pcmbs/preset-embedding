# @package _global_

# default configs for one-epoch training strategy

tags: 
  - ${task_name}
  - iter
  - ${dataset_train.name}
  - ${m_audio.name}
  - ${m_preset.name}

callbacks:
  model_checkpoint_step:
    every_n_train_steps: 100 # number of training steps between checkpoints

dataset_train:
  cfg: 
    dataset_size: 100000000000
    seed_offset: 1000

dataloader_train:
  batch_size: 64
  num_workers: 8
  shuffle: False
  sampler: 
    _target_: utils.data.SequentialSampler2
    

solver: # for ReduceOnPlateau
  scheduler:
    patience: 10 
  scheduler_config:
    monitor: train/loss 
    interval: step 
    frequency: 100 

trainer:
  min_steps: 10
  max_epochs: 1 # since a map-style dataset of size 100_000_000_000 is used
  
  # max_time: null # TODO: might be useful for SLURM jobs ?
  
  # set True to to ensure deterministic results
  # makes training slower but gives more reproducibility than just setting seeds
  deterministic: True
  
  # we control the validation frequency using val_check_interval since 
  # we use a one-epoch training strategy is used
  val_check_interval: null # TODO: update when validation implemented

# set hydra output directory
hydra:
  run:
    dir: ${paths.log_dir}/${m_preset.name}-${m_audio.name}/${dataset_train.name}-iter/${task_name}_${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ${paths.log_dir}/${m_preset.name}-${m_audio.name}/${dataset_train.name}-iter/${task_name}_${now:%Y-%m-%d}_${now:%H-%M-%S}