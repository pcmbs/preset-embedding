_target_: models.lit_module.PresetEmbeddingLitModule
_convert_: all # somehow resolves an omegaconf missing key error during trainer.fit setup

wandb_watch_args:
  log: gradients
  log_freq: 100

loss:
  _target_: torch.nn.L1Loss

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

# the following "???" values are define in base_{iter,epoch} experiments

# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1 # Factor by which the learning rate will be reduced
  patience: ??? # Number of `scheduler.step()` calls (see frequency below) with no improvement after which learning rate will be reduce

# https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers 
# the scheduler key-value pair is passed in the configure_optimizers method at runtime
scheduler_config:
  monitor: ??? # Metric to monitor for schedulers like `ReduceLROnPlateau` 
  interval: ??? # step or epoch (used by frequency below)
  frequency: ??? # How many epochs/steps should pass between calls to `scheduler.step()`. 